
day01:

LInux常用命令:
services.msc 命令行查看所有的服务
/ 代表根目录;
cd / 进入根目录	很多文件夹的哪个目录
cd ~ 进入root 目录 回家的意思,你是用root 登录的,普通用户只能回到home 文件下(回家)

dd  --删除当前行
ndd  --删除n行数据
dG  --删除当前后之后的全部行
rm -f 文件名	删除文件命令
mv -f f1 f2 移动文件或者文件夹到 指定目录;
mv name1 name2 给文件改名字;
y 赋值
p 粘贴

在这里，我们可以直接敲命令，	-- 展示行号
:set number 或者 :set nu 
然后回车


rm -rf ./hadoop-2.7.5/	删除某个文件夹

u撤销上一步操作
ctrl+r恢复上一步被撤销的操作

复制一行, 1,撤销编辑命令 2,yy 复制 3,p粘贴

ctrl+c可以终止ping , 也可以终止进程
ctrl+z可以暂停ping，该暂停只是把进程放到后台去了，使用命令fg可以调出到前台来

vi hello.txt 创建一个文件,或者打开一个文件.

mkdir 文件名, 创建单个的文件夹
mkdir -p xx/yy	的好处就是一次可以创建多级文件夹，若xx文件夹不存在，则先创建xx文件夹，
然后在xx文件夹下创建yy文件夹

pwd 打印当前工作目录，就是显示当前所在目录的意思。是print working directory的缩写

:q! 不保存文件并强制退出vi 编辑
:wq 保存文件并退出vi 编辑

find   find 路径 类型  xxx   find   /  -name    hello.txt	查找命令
df -lh 查看磁盘大小
history 查看执行的命令有什么
kill -9 14527 杀死进程

补充:
	grep -ir "string" .    #查找本目录以及本目录以下的文件	i 忽略大小写 , r 递归查询


===========================================================================================================
[hadoop01]
1.linux的命令巩固
2.shell编程
3.zk集群搭建

[hadoop02]
1.javaapi操作zk集群(//curator-framework)
2.hadoop1.x与2.x架构模型
3.Apache版的hadoop三种架构

[hadoop03]
1.CDH版的zk集群搭建
	*/ 选举机制(新集群|旧集群)
2.CDH版的hadoop伪分布式架构
3.hdfs的常用命令
4.hdfs的性能测试
5.hdfs的block块副本机制
6.hdfs的fsimage与edits文件合并 (//元数据管理)
	*/ 达到1H 超过64M
7.hdfs的文件读写过程

[hadoop04]
1.javaapi操作hdfs
2.mr入门程序
3.mr程序的两种运行方式

/////////////////////////////////////////////////////////////////////////////////////
[hadoop05]
1.mr程序的分区(partitioner)				hadoop04-cdh-mr/cn/itcast/demo1/partition
2.mr程序的规约(combiner)				hadoop04-cdh-mr/cn/itcast/demo2/sort
3.案例1:手机流量汇总求和				hadoop04-cdh-mr/cn/itcast/demo3/flowCount
4.案例2:手机上行流量排序				hadoop04-cdh-mr/cn/itcast/demo4/sortUpCountFlow
5.案例3:手机号码分区					hadoop04-cdh-mr/cn/itcast/demo5/phonepartition
6.mapTask运行机制(mapShuffle 分区 排序 规约)			
7.reduceTask运行机制(reduceShuffle 分组)

[hadoop06]
1.案例1:reduce端的join实现				hadoop04-cdh-mr/cn/itcast/demo6/reducejoin
2.案例2:map端的join实现					hadoop04-cdh-mr/cn/itcast/demo7/mapJoin
3.案例3:求共同好友						hadoop05-cdh-mr-yarn/cn/itcast/demo1/commonFriend
4.案例4:倒排索引						hadoop05-cdh-mr-yarn/cn/itcast/demo2/index
5.案例5:自定义TextInputFormat			hadoop05-cdh-mr-yarn/cn/itcast/demo3/inputformat
	*/ LineRecordReader
6.案例6:自定义TetxOutputFormat			hadoop05-cdh-mr-yarn/cn/itcast/demo4/outputformat
	*/ LineRecordWriter

[hadoop07]
1.案例1:自定义grouping求取top1
2.案例2:自定义grouping求取topN
3.mr的相关参数调整
4.yarn的资源调度策略

5.hive介绍与安装

理解mr代码 -> 清楚数据的流向 -> 编写代码 -> 描述 -> 解决问题
	
===========================================================================================================

rwx 读写执行 对应数字 421

虚拟机配置:
	vim /etc/udev/rules.d/70-persistent-net.rules
	vim /etc/sysconfig/network-scripts/ifcfg-eth0 

	00:50:56:35:12:8E
	00:50:56:21:57:57
	00:50:56:29:90:07

	IPADDR=192.168.83.110
	NETMASK=255.255.255.0
	GATEWAY:192.168.83.2
	DNS1=8.8.8.8

	reboot  -h   now 重启虚拟机
	shutdown -h now 关闭虚拟机
	
三台虚拟机的hostname  node01  node02  node03   
192.168.xxx.110 
192.168.xxx.120  
192.168.xxx.130


vim /etc/sysconfig/network 该虚拟机名字

kill -9 pid 杀掉进程  
-l list的意思

:set nu 显示行号 

处理变量不空,其他倒出都空格;
脚本语言:所见即所得;


echo "scale=4; 3/2" | bc 求小数

` ` 飘号, 把里面的字符当作命令执行;
> 表示覆盖,  >>表示追加

qa queryALl 查询

tail -f hello.txt 动态的查询文件的尾巴

scp 远程copy s代表ssh 安全链接协议

$path 对命令的引用

scp -r  /export/servers/zookeeper-3.4.9/ node02:/export/servers/
scp -r  /export/servers/zookeeper-3.4.9/ node03:/export/servers/

#!/bin/bash
a=10
b=20
if [ $a -eq $b ]
then
   echo "$a -eq $b : a 等于 b"
else
   echo "$a -eq $b: a 不等于 b"
fi



a=10
b=20
if [ $a -gt $b ]  
then
        echo 'a>b'
else
        echo 'a<b'
fi
学会循环,判断语句,就对一门语言入了门....

zkeeper 特性:全局数据一致性
zk :2888:3888 内部通信端口
leader
follower


tar -zxvf hadoop-2.7.5.tar.gz -C ../servers/ 解压tar 包
cp zoo_sample.cfg zoo.cfg  拷贝一个文件并改名
echo 1 > /export/servers/zookeeper-3.4.5-cdh5.14.0/zkdatas/myid	创建myid 文件,并往里面存入简单的值


开启zookeeper:
					     jps
/export/servers/zookeeper-3.4.5-cdh5.14.0/bin/zkServer.sh start

关闭
/export/servers/zookeeper-3.4.5-cdh5.14.0/bin/zkServer.sh stop
查看状态
/export/servers/zookeeper-3.4.5-cdh5.14.0/bin/zkServer.sh  status

ps -ef | grep zookeeper  查看zookeeper的详细信息

yarn 资源管理平台

start-dfs.sh  sbin下执行此命令可以启动当前虚拟机下的 namenode datanode secondaryname

启动Hadoop的服务器
cd /export/servers/hadoop-2.6.0-cdh5.14.0/
	sbin/start-dfs.sh
	sbin/start-yarn.sh
	sbin/mr-jobhistory-daemon.sh start historyserver	JobHistoryServer:这是yarn提供的一个查看已经完成的任务的历史日志记录的服务，我们可以启动jobHistoryServer来观察已经完成的任务的所有详细日志信息
	

停止Hadoop服务器:	
	cd /export/servers/hadoop-2.7.5
	sbin/stop-dfs.sh
	sbin/stop-yarn.sh
	sbin/mr-jobhistory-daemon.sh stop historyserver
	

远程发送:
	cd  /export/servers/
	scp -r hadoop-2.7.5 node02:$PWD
	
standby 备份	
脚本一键启动 : 推荐
	如果配置了 etc/hadoop/slaves 和 ssh 免密登录，则可以使用程序脚本启动所有Hadoop 两个集群的相关进程，在主节点所设定的机器上执行。
启动集群
	node01节点上执行以下命令
	第一台机器执行以下命令
	cd /export/servers/hadoop-2.6.0-cdh5.14.0/
	sbin/start-dfs.sh
	sbin/start-yarn.sh
	sbin/mr-jobhistory-daemon.sh start historyserver
停止集群：没事儿不要去停止集群
	sbin/stop-dfs.sh
	sbin/stop-yarn.sh
	sbin/mr-jobhistory-daemon.sh stop historyserver
	
day02:	
	hdfs常用的操作命令:
	
	hdfs dfs -put /a.txt	 /	上传文件到hdfs
	yarn jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar JobMain 运行jar包 包名+主类全路径(我的这个实在java目录下的) 指定要运行的主类
	hdfs  dfs   -ls  /  查看根路径下面的文件或者文件夹
	hdfs dfs  -mkdir  -p   /xx/xxx  在hdfs上面递归的创建文件夹
	hdfs  dfs -moveFromLocal  sourceDir(本地磁盘的文件或者文件夹的路径)   destDir（hdfs的路径）  
	hdfs  dfs  -mv  hdfsSourceDir   hdfsDestDir
	hdfs  dfs -put  localDir  hdfsDir   将本地文件系统的文件或者文件夹放到hdfs上面去
	hdfs  dfs -cat  hdfsDir 查看hdfs的文件内容
	hdfs  dfs  -cp   hdfsSourceDIr   hdfsDestDir   拷贝文件或者文件夹
	hdfs  dfs  -rm   [-r]  （递归）删除文件或者文件夹
	hdfs的权限管理两个命令：
	hdfs  dfs  -chmod -R  777  /xxx
	hdfs  dfs  -chown  -R hadoop:hadoop  /xxx
	
	cat
	put
	ls
	rm
hadoop常用命令(之前用的):
	hadoop fs -ls /
														
	控制策略: 时间长度, 文件大小
	副本机制
	机架感知原理
	统一命名空间
	yarn资源调度系统

	node01:8020
	改名 mv 文件1 文件2
	
	监控map reduce的执行文件夹的状态
	心跳机制, 改一下



补充: IDEA快捷键:
	idea的解决问题的快捷键: alt+enter
	ctrl+f12 查找方法
	ctrl+shift+alt+n	快速找到一个类或者方法
	iter 增强for循环
	100.fori 打印0~100 循环
	查看接口的实现类	IDEA 风格 ctrl + alt +B     或者     Ctrl+Alt+鼠标左键
	ctrl+h 查看类的结构图
	alt + insert 构造方法, set ,get方法
	
	Method signature 方法名
	
	          
	备份
	
	人力外包,项目外包
	
	rpc远程调用服务,让使用者感觉即使在本地调用一样,其实是远程调用, 底层是socket实现的			        
	
	
=========================================================
day04: MapReduce的流程:

map阶段的两个步骤
1、第一步：读取文件，解析成key,value对  这里的key，value对指代的是k1  v1	k1指行偏移量(上一行的字节数) k2指的是一行数据
2、第二步：自定义map逻辑，接收第一步读取的k1,v1 ，转换成新的k2,v2输出

shuffle阶段四个步骤
3、第三步：分区  满足条件的数据发送到同一个reduce里面去, 这里的key指代的是k2
4、第四步：排序  对我们的数据进行字典顺序的排列
5、第五步：规约  主要是在map端对数据做一次聚合，减少我们输出的k2的数据量
6、第六步：分组  将相同的数据发送到同一组里面去调用一次reduce逻辑,形成一个集合	主要作用就是形成集合

reduce阶段的两个步骤
7、第七步：自定义reduce逻辑，接收k2  v2  转换成新的k3  v3 进行输出
8、第八步：输出  将我们reduce处理完成之后的数据进行输出

八个步骤背下来

每一个步骤都是一个class类，将八个步骤的class类组织到一起就是我们的mapreduce的程序

<K2,V2>根据业务需求定义的


===========================================================================================================
MapTask运行的整个过程  背下来
1、TextInputFormat读取数据
2、调用map逻辑，默认是一个切片（就是一个block块）对应一个mapTask
3、数据写入到环形缓冲区，默认环形缓冲区的大小是100M，环形缓冲区其实就是一个数组
4、数据一直往环形缓冲区当中写，数据在环形缓冲区当中实现分区，排序，规约，分组等
5、等到数据写到环形缓冲区的80%的时候，启动溢写线程，将内存当中80M的数据，溢写到磁盘上面去
6、等到maptask完成之后，磁盘上面可能存在很多的小文件，这些小文件已经做好了局部排序，分区，规约等步骤，再把这些小文件合并成一个大的文件
7、等待reduce阶段来拉取这个文件

ReduceTask的运行的整个过程  背下来
1、启动线程到mapTask那里去拷贝数据，拉取属于每一个reducetask自己内部的数据
2、数据的合并，拉取过来的数据进行合并，合并的过程，有可能在内存当中，有可能在磁盘当中，有可能在内存和磁盘当中，合并的时候同时要进行分组操作
3、调用reduce逻辑
4、数据输出


[hadoop05]
1.mr程序的分区(partitioner)				hadoop04-cdh-mr/cn/itcast/demo1/partition
2.mr程序的规约(combiner)				hadoop04-cdh-mr/cn/itcast/demo2/sort
3.案例1:手机流量汇总求和				hadoop04-cdh-mr/cn/itcast/demo3/flowCount
4.案例2:手机上行流量排序				hadoop04-cdh-mr/cn/itcast/demo4/sortUpCountFlow
5.案例3:手机号码分区					hadoop04-cdh-mr/cn/itcast/demo5/phonepartition
6.mapTask运行机制				
7.reduceTask运行机制	
===========================================================================================================


逻辑标识
分区??第二种方法	转化一下输出类型
对谁比较, 把谁封装成key

规约处理单词计数案例??
规约, 缺最大值,求和的时候可以用





补充:
	hdfs文件的写入过程：搞定  
	hdfs的文件的读取过程：搞定
	HDFS的javaAPI操作  搞定  FileSystem
	
	恢复hdfs垃圾箱文件恢复命令??
	hdfs文件的写入过程??
	hdfs文件的读取过程??

===========================================================================================================
day05__hadoop
	bin/hadoop checknative 检查支持的软件
	在hdfs中大文件分成若干个块, 在mapreduce中,一个块被叫做 一个切片
	排序,对象的比较
	数据类型 	
	sequenceFile 一种文件格式

hdfs://node01:8020/wordcount/out java代码上传的地址
	
分区只能在集群上跑,window上只是伪分布式集群	
分组: 定义的javaBean必须重写toString 方法,会自动调用,在mapper,,context.write()输出中,输出javaBean 默认会调用toString方法, 会直接输出想要的字段
分组的作用: 是将相同的key2进行合并,对应的value形成一个集合,去一起调用reduce逻辑


分区与分组的区别：
	分区：决定了我们的数据去到哪一个reduce里面去
	分组：决定了我们一个reduce内部的数据，哪些作为一个集合  相同key的数据对应的value形成一个集合
			
	
	
补充:
	java的Api操作hdfs, 读取文件????
	切片是什么??
	自定义输入,输出,按老师的输入 传入context做一下


要求:理解mr代码->清楚数据的流向 ->编写代码 -> (和同学)描述 ->解决问题????

===========================================================================================================

day06_hive
	先找会的弄明白

时变性：根据一些不同的指标求取，会产生不同的一些分析维度 长方体例子

hive=== 工具, 说白了就是:mapreduce的客户端. 可以把sql语句转化为mapreduce语句

元数据库：hive的元数据  表与hdfs数据之间的映射关系 默认使用的是derby，一般都会改成mysql


安装方式:二进制安装
删进程: ps -ef |grep hive		对于mysql 	ps -e |grep mysql也行	
kill -9 进程号

ETL L加载,一般会加载到数据库中...	
bin/hive -e "use myhive;select * from test;" -e执行一条hql语句, -f 执行文件

使用mysql共享hive元数据

第二种交互方式：Hive JDBC服务 推荐
	后台启动:
			cd  /export/servers/hive-1.1.0-cdh5.14.0
			nohup bin/hive --service hiveserver2  & 	执行完之后,要记得按enter键 输入下一行命令
		
	beeline连接hiveserver2
			bin/beeline
			!connect jdbc:hive2://node03:10000
	然后操作数据仓库,hive
			
删除数据库
	删除一个空数据库，如果数据库下面有数据表，那么就会报错
		drop  database  myhive2;
	
	强制删除数据库，包含数据库下面的表一起删除 (暴力)
		drop  database  myhive  cascade;   不要执行了
查看数据库基本信息
	desc  database  myhive2;
查看数据库更多详细信息
	desc database extended  myhive2; 
展示表信息:	show tables;
删除一个表:	使用drop语句，drop table （需要删除表的名字）。
			
查询表的类型:	desc formatted  stu2;

show functions;
--设置hive的本地运行模式	set hive.exec.mode.local.auto=true;	 ===================================================

从本地文件系统向表中加载数据	load data local inpath '/export/servers/hivedatas/student.csv' into table student;
加载数据并覆盖已有数据 			load data local inpath '/export/servers/hivedatas/student.csv' overwrite  into table student;

hive表当中加载数据：
	load  data 加local从本地加载, 不加local 从hdfs上加载, 从hdfs上加载本质是:文件的剪切;
		通过load的方式加载数据  load data local inpath '/export/servers/hivedatas/score.csv' overwrite into table score partition(month='201806');

	insert overwrite  select	一般把查询的语句放在另一张表中;
	 通过一张表，然后将查询结果插入到另外一张表里面去	insert overwrite table score4 partition(month='201802') select s_id ,c_id ,s_score from score;
	
通过查询插入数据
	通过load方式加载数据
	load data local inpath '/export/servers/hivedatas/score.csv' overwrite into table score partition(month='201806');

通过查询方式加载数据
	create table score4 like score;
	insert overwrite table score4 partition(month = '201806') select s_id,c_id,s_score from score;		
	
limit 10 展示10条数据

查看分区	show  partitions  score;

加载数据到 /**指定文件夹下**/: 不能写根目录,会全部格式化*********
insert overwrite local directory '/export/servers/hivedatas/sort' select * from score distribute by s_id sort by s_score;

自定义函数  UDF
	list jar 展示正在使用的jar包
	添加我们的jar包
		重命名我们的jar包名称
		cd /export/servers/hive-1.1.0-cdh5.14.0/lib
		mv original-day_06_hive_udf-1.0-SNAPSHOT.jar udf.jar
	hive的客户端添加我们的jar包
		add jar /export/servers/hive-1.1.0-cdh5.14.0/lib/udf.jar;
	设置函数与我们的自定义函数关联
		create temporary function tolowercase as 'cn.itcast.udf.ItcastUDF';
	使用:
		select tolowercase('abc');
		
hadoop支持什么压缩形式:	bin/hadoop checknative;

分桶表: 要执行mapreduce, 是要通过mr进行计算, load的本质:load data的本质就是移动hdfs文件到指定表目录中
		不能用load加载, 只能用insert overwrite table XXX hql语句;

hive的数据存储格式:
	行式存储:textFile sequenceFile;
	列式存储:orc parquet(cloudera公司的)
第二种方式连接,查看mr进程;;; 
小任务模式, 本地运行模式;set hive.exec.mode.local.auto=true

分组: group by 是在map阶段进行的,相当于 k2(按某个字段进行分组),全局分组;

数据的流向; 定时任务调度; 	图片手机(离线项目流程)	抽取,转化, 加载(ETL)
echo $JAVA_HOME;	rand(123)值固定了	 0.7231742029971469 
推测执行?? 一般关闭推测执行, 就好比:一个mr任务运行到90%卡在那了,推测执行机制会认为任务错误,会重新产生一个MapReduce任务,重新执行,还是会卡在那,运行不了,一般关闭..



	
=========================================
补充:
	查看网址???
	日期解析:	18/Sep/2013:06:49:18		2013-09-18 07:10:50	试一下
			public static SimpleDateFormat df1 = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss", Locale.US);
			public static SimpleDateFormat df2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss", Locale.US);
			df2.format(df1.parse(time_local));
	
	snappy压缩: 试一下???????
	
		

18/Sep/2013:07:01:57 转一下日期
  //格式化时间方法
    public static String formatDate(String time_local) {
        try {
            return df2.format(df1.parse(time_local));
        } catch (ParseException e) {
            return null;
        }

    }
public static SimpleDateFormat df1 = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss", Locale.US);
public static SimpleDateFormat df2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss", Locale.US);

cn.itcast.hive.myudf.StringToDate
create temporary function stringtodate as 'cn.itcast.hive.myudf.StringToDate';


动态分区 (搞一下):	常用函数的使用
insert overwrite table log10 partition(time) 
SELECT track_time, url,session_id,referer,ip,end_user_id,city_id, 
CASE WHEN  track_time  BETWEEN '2017-08-10 13:00:00' AND '2017-08-10 13:10:00' THEN '2017-08-10 13-00-00'
WHEN  track_time  BETWEEN '2017-08-10 13:10:00' AND '2017-08-10 13:20:00' THEN '2017-08-10 13-10-00' 
WHEN  track_time  BETWEEN '2017-08-10 13:20:00' AND '2017-08-10 13:30:00' THEN '2017-08-10 13-20-00' 
ELSE '2017-08-10 13-30-00' END time  FROM log1;

===========================================================================================================
flume: 数据采集


nohup XXX & 把应用挂到后台使用; ???		如 nohup bin/hive --service hiveserver2  &          后台启动hive服务
a1 agent名字, 一个flume 是一个实例,(一个agent)

看一下执行命令代表什么意思：-c 当前文件夹 相对路径 -n 实例的名字  打印到控制台；
 bin/flume-ng agent -c ./conf -f ./conf/spooldir.conf -n a1 -Dflume.root.logger=INFO,console

涉及到网络拷贝:	用avro 类型	第9个视频	envent 即是流经channel的一条条数据,多条数据组成events,就是一个flume的实例agent,
数据的脱敏: 脱掉敏感信息(例如:手机号加密)          provided: 开发的时候要,打包的时候不要...

第二种拦截器,java代码,跑一下
azkaban 工作流调度器;

tail -F +文件名,可以动态的监听文件的变化
round_robin,轮循机制 (你一条,我一条)			 random 随机机制;
execute flow 执行工作流;	schedule 定时执行

启动mysql: /etc/init.d/mysqld start | status 查看状态	强烈推荐yum安装;
			whereis init.d


补充:

看过就是会,写过就是熟;
在设置变量时,等号两边不能加空格		bin/flume-ng agent -c conf -f conf/tail-file.conf -n a1 -Dflume.root.logger=INFO,console	
开启azkaban: 最好不要在bin目录下启动,可能会启动不起来
	cd  /export/servers/azkaban-solo-server-0.1.0-SNAPSHOT
	bin/start-solo.sh

cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin
bin/flume-ng agent -n a1 -c conf -f conf/load_banlancer_server.conf -Dflume.root.logger=DEBUG,console
===========================================================================================================
sqoop数据迁移和离线项目:

两个问题: 1,防火墙 2,远程连接
开启mysql的远程连接权限:
	进入mysql的客户端然后进行授权:
			GRANT ALL  PRIVILEGES ON *.*  TO 'root'@'%'  IDENTIFIED BY 'yourpassword' WITH  GRANT  OPTION;
			FLUSH  PRIVILEGES;
	
	bin/sqoop list-tables --connect jdbc:mysql://192.168.83.5:3306/5.22 --username root --password 123456
访问次所与session有关, 默认半个小时是在同一个session里;

两个角度分析:	1,页面访问角度 (pageView)
				2,用户的角度(visit)


补充:
	通用命令: common command
	知道jar包,怎样构建 mvn的jar包;
	看离线流程图写内容; 
	上一天的flume:静态拦截器
	https://coderschool.cn/1853.html
	https://blog.csdn.net/Abysscarry/article/details/81408265
	窗口函数: 降序排序, 从下往上看;
	//建立maven坐标,命令;
	mvn install:install-file -Dfile=sshxcute.jar -DgroupId=org.clojars.jkandasa -DartifactId=sshxcute -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar
	求topN hql函数???
	练一下窗口函数...
第八天:建表语句
	H:\黑马资料\06#大数据\08、网站的数据采集\资料\网站流量日志分析：项目脚本-配置代码\

	飘号: 在shell脚本中要执行Linux命令, 用 `` 号(飘号) 	`date -d'-1 day' +%Y%m%d`
	sqoop导出数据,hive可以不启动, 在本地mysql中需要创建对应的表结构....
	
	删除数据库:	drop  database  myhive  cascade; (暴力,连表一起删除)
===========================================================================================================

impala:
	hdfs的短路读取;(要读取的文件在同一块磁盘上,直接读取,速度变快)
	
	impala-catalog
	impala-state-store
	impala-server
启动hive服务: 
	nohup bin/hive --service metastore &		简写: 这个服务必须开,因为我们hive的元数据存储在mysql上
	nohup bin/hive --service hiveserver2 &				 nohup /bin/hiveserver2 &  后台启动;
		
启动impala服务:  impala

	1,主节点node03启动以下三个服务进程:
		service impala-state-store start
		service impala-catalog start
		service impala-server start

	2,从节点启动node01与node02启动impala-server
		service  impala-server  start
		查看impala进程是否存在
		ps -ef | grep impala
注意：启动之后所有关于impala的日志默认都在 /var/log/impala 这个路径下，node03机器上面应该有三个进程，node02与node01机器上面只有一个进程，
如果进程个数不对，去对应目录下查看报错日志

	3、重新启动hue的服务
			cd /export/servers/hue-3.9.0-cdh5.14.0/
			build/env/bin/supervisor
			
	4.启动与关闭oozie服务
		启动命令
			cd /export/servers/oozie-4.1.0-cdh5.14.0
			bin/oozied.sh start 
		关闭命令
			bin/oozied.sh stop
	
	
	
	
	
	
补充:
	df -lh 查看虚拟机磁盘大小;
	Linux的垃圾桶: lost+found 失物招领
nohup bin/hive --service metastore &
nohup bin/hive --service hiveserver2 &
	检查服务启动成功没??? tail -200f nohup.out
D://盘 11个mr程序;

测试MapReduce任务: hadoop jar /export/servers/hadoop-2.6.0-cdh5.14.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.0.jar pi 3 5

hdfs上给文件加权限:	hdfs  dfs  -chmod o+x /tmp

impala底层使用c++实现的,对应hive当中的udf(自定义函数) impala不兼容,  -- hive中的自定义函数是用java代码实现的;

df -lh 查看虚拟机磁盘大小;
free -m 查看虚拟机运行内存大小
echo 1 > /proc/sys/vm/drop_caches 释放内存
修改环境变量后要刷新生效:	source /etc/profile

注意:
	copy的时候,删除两个单词tab键补充一下;
	
	





Kafka:
	是什么:
		1）Apache Kafka 是一个消息队列（生产者消费者模式）
		2）Apache Kafka 目标：构建企业中统一的、高通量、低延时的消息平台。
		3)大多的是消息队列（消息中间件）都是基于JMS标准实现的，Apache Kafka 类似于JMS的实现
				

kafka高度依赖zookeeper...
Kafka Topic: 主题，一类消息的名称。存储数据时将一类数据存放在某个topci下，消费数据也是消费一类数据。

1, broker.id log.dir zookeeper.id 
2. kafka作用: 缓冲 | 异构系统 一个项目拆成多个模块 | 解耦合
		怎样理解缓冲呢? 当大量数据一起发过来的时候,服务器容易雪崩, kafka相当于中间件(中间件,中介),把数据发送到kafka中先缓冲起来,需要特定消息的程序
						从kafka中取出来消费,起到缓解服务器压力的作用,缓冲...
		解耦系统: 一个项目分为多个模块,模块直接有依赖关系,比如电商网站的商品模块和订单模块,订单需要商品的数据,这时候可以把山坡模块的数据发到kafka上
				  什么时候产生订单可以从kafka上取出相应的商品信息,解除模块直接的耦合关系.
3. 创建一个topic指定分片和副本
4. kafka数据以文件的形式存储在磁盘上;
5. 怎样分片??	
		分片:当数据量非常大的时候，一个服务器存放不了，就将数据分成两个或者多个部分，存放在多台服务器上。每个服务器上的数据，叫做一个分片。
6.  确认符: ack


补充:
	
启动kafka:
	cd /export/servers/kafka/bin
	./kafka-server-start.sh /export/servers/kafka/config/server.properties
	脚本命令 配置文件
	
Linux解压zip文件: unzip 
	unzip ./kafka-manager-1.3.3.17
查找配置文件(非注释内容) 反选 -v 管道:	cat server.properties |grep -v "#"
后台启动kafka web服务: nohup ./kafka-manager & 后台
					   bin/kafka-manager 前台
					
					   
	

hive 分区,分桶 ??????
	分区表：
		在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，
		同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天，或者每小时进行切分成一个个的小的文件，这样去操作小的文件就会容易得多了
	分桶表:
		分桶表
将数据按照指定的字段进行分成多个桶中去，说白了就是将数据按照字段进行划分，
可以将数据按照字段划分到多个文件当中去

ll -h 可以展示文件大小

70万条数据 约等于 1G磁盘大小	1000000KB 万条

3. 思考: flume与kafka整合????

4. df -lh 查看虚拟机磁盘大小;
5. free -m 查看虚拟机运行内存大小
6. echo 1 > /proc/sys/vm/drop_caches 释放内存
7. 修改环境变量后要刷新生效:	source /etc/profile

8. 注意:
	copy的时候,删除两个单词tab键补充一下;
9. 自定义maven仓库包: hadoop07包
		<!--mvn install:install-file -Dfile=sshxcute.jar -DgroupId=org.clojars.jkandasa -DartifactId=sshxcute -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar-->





==================================================================================================================================
Hbase:
	
	zookeeper:分布式应用程序协调服务(管理员)	小文件存储系统;
	2. 比较过滤器
	3. 
	
	
	
	
	
补充:

	typora软件 打开md文件
	反选命令试一下??? cat  hbase-env.sh | grep -v "#"
	flume和kafka的整合???
	1. 有没有查询 hbase的 sql语句
	2. qualified  ctrl alt + m  
	3.整理集群的办法: 
	   zkdatas 留下myid
	   hdfs ,hdfsdatas,格式化即可
==================================================================================================================================	   

Scala:
	1. 静态编译
	2. 九九乘法表
			for(i <- 1 to 9; j <- 1 to 9 if i>=j) { print(j + "*" + i + "=" + i*j + "\t"); if(i==j) println() }
	3. 变量
	//使用val定义的变量值是不可变的，相当于java里用final修饰的变量
	 val i = 1
   //使用var定义的变量是可变得，在Scala中鼓励使用val
	var s = "hello"
    4. 循环:
		 for (i <- 1 to 10)
		 println(i)
		//for(i <- 数组)
		val arr = Array("a", "b", "c")
		for (i <- arr)
		  println(i)
	 5.  //高级for循环
			//每个生成器都可以带一个条件，注意：if前面没有分号
			for(i <- 1 to 3; j <- 1 to 3 if i != j)
			  print((10 * i + j) + " ")
			println()

			//for推导式：如果for循环的循环体以yield开始，则该循环会构建出一个集合
			//每次迭代生成集合中的一个值
			val v = for (i <- 1 to 10) yield i * 10
			println(v)

	6.  val arr = ArrayBuffer(("aa",1),("aa",2),("aa",3));
		arr.foreach(x => println(x._1+"\t"+x._2))
		取元组的值的操作,变量._n (一般 是对偶元组)
	7: 函数定义
		val f1 = (x:String)=> println(x) 函数是一个对象 一行可以省略{}
		全称: val f2 = (x:String,y:Int)=> {函数体}
补充：	 case 模式匹配， 匹配上了就返回 后面的执行逻辑；

			 /*
				 val arr = Array("hadoop", "zookeeper", "spark", "storm")
				  //随机取数组中的一位，使用Random.nextInt
				  val name = arr(Random.nextInt(arr.length))
				  println(name)
				  name match {
					case "hadoop" => println("大数据分布式存储和计算框架...")
					case "zookeeper" => println("大数据分布式协调服务框架...")
					case "spark" => println("大数据分布式内存计算框架...")
					case _ => println("我不认识你")
				  }
			 */
		
		
		
		
		
		

02:scala
 消息发送的方式:
	!	发送异步消息，没有返回值。
	!?	发送同步消息，等待返回值。
	!!	发送异步消息，返回值是 Future[Any]。
	
	
补充:
	一行代码不用加{} 多行要加
	def fun1(s:String):Unit = {print(s)}
	filter() 里面是boolean值
	
	1. 神奇的下划线 _
		1.将方法转成函数
		2.元组中取数据  t._1   t._2   t._3 (取元组中第几个元素)
		3. 当对象调用某个方法的时候，方法中本来是传入的是一个匿名函数，可以使用_简化改写
		4.导包   import xxxxxxx._
	2. ctrl + alt + v 添加返回值
	3. 什么是特定顺序???? 从左到右, 或者从右到左
	4. val map = Map ("a"->1,"b"->2) 必须加 引号
	   或者: val map = Map (("a",1),("b",1))
	5. 异步: 并行计算, 同步计算,一个一个计算
scala02:
	1. 构造器改下参数
	2. 参数名: 参数类型			方法名: 方法的返回值类型
	ctrl + i (实现)重写未实现的方法, 不然报错.类似接口
	main方法不能定义在class里面, 需要 object中
	偏函数
	方法里面调方法, 只不过方法是匿名方法
	scala中的get set方法,就相当于 实例.属性名
	
03 scala:
	map方法作用于(数组中)每个元素
	匿名函数: 没有函数名,只给出函数的实现
	
	
	


补充:
	1. 函数在scala中表示一个对象, 用 val 定义
		val f1 = (x:Int)=>x* 函数可以用一个对象接收
	2. 方法传参, 可以是一个函数(函数可以看出一个对象),不能是个方法 作用点: 参数   
		方法中可以调用方法, 作用范围: 方法体
		
	块表达式: 定义变量时用 {} 包含一系列表达式，其中块的最后一个表达式的值就是块的值。
	scala特定的解析方式: 前面要加个 s (特殊的拼串方式 $+变量名)
	 
	

=====================================================================================================================================	
Spark:

01:spark
优点: 
	1. 通用 在一个程序中,可以用很多spark的组件	Spark 提供一个完整的技术栈, 包括 SQL执行, Dataset命令式API, 机器学习库MLlib, 图计算框架GraphX, 流计算SparkStreaming
												用户可以在同一个应用中同时使用这些工具, 这一点是划时代的

	2.Spark-Core 是整个 Spark 的基础, 提供了分布式任务调度和基本的 I/O 功能
	total-executor-cores 2 \ 任务的并行度
	1. 编写代码的两种方式	spark-shell spark-submit 用于 测试 和 探索
	大规模分布计算引擎
	rdd是什么???? 1.数据集 2.编程模型 3.rdd之间有联系 4.rdd可以分区
	
	
	
	
	
val conf = new SparkConf().setMaster("local[6]").setAppName("debug_string")
val sc = new SparkContext(conf)
val interimRDD = sc.textFile("dataset/access_log_sample.txt")
  .map(item => (item.split(" ")(0), 1))
  .filter(item => StringUtils.isNotBlank(item._1))
  .reduceByKey((curr, agg) => curr + agg) 

val resultLess = interimRDD.sortBy(item => item._2, ascending = true).first()
val resultMore = interimRDD.sortBy(item => item._2, ascending = false).first()

println(s"出现次数最少的 IP : $resultLess, 出现次数最多的 IP : $resultMore")

sc.stop()
	







补充:
	结构化数据和半结构化数据:
		
	吞吐量: 吞:对大量数据的处理,一次能处理多大的数据量 吐:数据处理的速度
	RDD(Resilient Distributed Datasets) 弹性分布式数据集，
	rdd1.collect() 搜集一下文件内容
	
	 echo $JAVA_HOME 	查看JAVA_HOME环境变量
	 Seq 相当于 List集合
	 RDD[Array[String]] 只能说明rdd存放的 /*数据类型*/ 是 Array[String] , val rdd1:RDD[Array[String]] rdd1代表的是一个List集合
	
	idea把一行上下移动: alt + shift + 上,下键
名词解释
	Driver
		该进程调用 Spark 程序的 main 方法, 并且启动 SparkContext
	Cluster Manager
		该进程负责和外部集群工具打交道, 申请或释放集群资源
	Worker
		该进程是一个守护进程, 负责启动和管理 Executor (相当于一台服务器)
	Executor
		该进程是一个 /*JVM虚拟机,*/ 负责运行 Spark Task
		
求topN是每组里面的topN , 相当于每个订单的 所买商品的前几个最大值
/*分区与分组的区别：
	分区：决定了我们的数据去到哪一个reduce里面去
	分组：决定了我们一个reduce内部的数据，哪些作为一个集合  相同key的数据对应的value形成一个集合
*/	
02:spark
	
	1. .take(10) 出前10项记录
	2.如何放在集群中运行? 
		简单来讲, 并行计算就是同时使用多个计算资源解决一个问题, 有如下四个要点
		要解决的问题必须可以分解为多个可以并发计算的部分
		每个部分要可以在不同处理器上被同时执行
		需要一个共享内存的机制 (某些数据需要共享,程序才能顺利执行)
		需要一个总体上的协作机制来进行调度 (调度平台 如: yarn)  Spark Standalone Hadoop Yarn Apache Mesos Kubernetes

	3. 存储侧, 计算侧 分区 (rdd是可以分区的)
	
	/ 什么叫做弹性分布式数据集 ***/
		分布式
		RDD 支持分区, 可以运行在集群中

		弹性
		RDD 支持高效的容错

		RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中

		数据集
		RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数

		RDD 也可以缓存起来, 相当于存储具体数据
	4.Distinct 算子用于去重
		注意点
		Distinct 是一个需要 Shuffled 的操作

		本质上 Distinct 就是一个 reductByKey, 把重复的合并为一个
/*CombineByKey 这个算子中接收三个参数:*/
    
	转换数据的函数(初始函数, 作用于第一条数据, 用于开启整个计算), 在分区上进行聚合, 把所有分区的聚合结果聚合为最终结果

	缓存的意义:
		1.提高数据的利用率
		2.减少代码的重复执行，提高代码执行效率
		3.容错的一种手段
	缓存的api: cache , persist   cache相当于缓存级别为仅把中间结果存储在内存中的persist  缓存级别为(MEMORY_ONLY)
			persist可以设置缓存级别, 接在磁盘上存储, 在内存中存储, 在内存中存储并序列化

checkPoint: 
	1.checkpoint意义
	1.方式
		1.可靠的：hdfs
		2.本地
	2.什么叫斩断依赖链
		类比Hadoop中Edits和FSImage的处理方式
	3.和cache的区别
		1.checkpoint把数据存储到可靠的hdfs系统中，而cache和persist把数据存储到内存和磁盘中
		2.checkpoint可以斩断依赖链，而cache和persist不行
		3.checkpoint没有向上的依赖链，数据存储到了hdfs上，所以程序结束后数据依然存在，但是cache和persist在程序结束后会被清除
2.checkpoint API
	1.在使用checkpoint之前需要先通过sc.setCheckpointDir(path="")指定数据存储目录
	2.调用checkpoint
	3.为什么在checkpoint之前先cache？
		因为checkpoint会重新计算整个RDD依赖链的，而其后如果调用action算子，则该任务会被调用两次，先cache的话整个任务之后出发一次作业提交的动作，提高运算效率




补充:
	reduceByKey 两个两个的聚合
	重命名快捷键: shift+f6
	idea区分大小写;
	collect: 把 RDD 当成一个容器, collect 相当于把容器打开,数据取出来,然后再用scala的foreach 方法遍历(不写collect可能取出来的是数组的引用)
	.setAppName() 相当于给提交的任务指定唯一的标识(防止多个任务时混乱)
	Checkpoint: 直接把数据存在hdfs上, 需要指定hdfs的存放的目录
	RDD的特性??? //重要
	RDD 针对的是数据集的计算,不针对某条数据,, Rdd是只读的
	
	1. Partition List 存放分区信息
	优先计算: 把计算的逻辑放到要处理数据的附近
		属性5:	Preferred Location 优先位置, 为了实现数据本地性操作, 从而移动计算而不是移动存储, 需要记录每个 RDD 分区最好应该放置在什么位置
	2. rdd的容错:	1.重新计算 2.缓存
	
		rdd.**ByKey 必须是 key-value的数据结构 ************************
	
	 def reduceByKey(func: (V, V) => V): RDD[(K, V)] ={} 
	 // func: (V, V) => V func:看做方法名, : 后面看做方法体 (当方法的参数是一个函数时的读取情况, 方法的参数可以是函数,函数是一个对象,可以传进去,
	 参数不能是方法)
	 3. 由于各种原因会导致源码看不到,删库重新下
=====================================================================================================================================	 


03:Spark
	1. 分区侧重于计算, 分片侧重于存储, 都是一个意思
	2. 至此便可得出cache和persist的区别了：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。








补充:

	看源码:
		1.只看能看懂的
		2.参数,返回值,逻辑, 联想
	隐式转换???*****
	启动spark命令: bin/spark-shell --master local[6] 分区数为6
	堆外内存: 脱离了jvm 管辖之外的内存;
	缓存前是否序列化: 当文件过大时,考虑是不是先序列化再存入存储设备(会节省内存空间),磁盘中对象只能以序列化的形式存在,对象可以直接存储在内存中
		
=====================================================================================================================================	
04 Spark

		1. RDD 的逻辑执行图:	表示 RDD 之间的关系, 数据的流转过程. (数据计算过程)
			物理执行图: 怎样把作业切分,放在集群中运行
		2. 宽窄依赖: 判断rdd是否能在一个流水线上执行(task)
		

补充:
	求top10 PM
	每次checkpoint 算子会重新计算, 在调用checkpoint之前会调用cache ,两个都会提交任务...
	Scala字符串特有的匹配方式: s"${item._1},${item._2}"		hive,2
	作业提交.....
	:paste ----> shift + d 复制,执行 ctrl + d
	job 从计算到求得结果的过程
	
	数据流向从前往后
	地图????????
	集合追加 ++= 数字累加+=
	查看接口的实现类  ctrl + alt+ B /// 或者 ctrl+alt + 鼠标左键	ctrl + 鼠标左键是所有使用的地方





	
	
	
=====================================================================================================================================	
	
05: SparkSOL:
	1. SparkSQL 是一个为了支持 SQL 而设计的工具, 但同时也支持命令式的 API
	dataset和dataframe
		rdd的处理数据思路:是把整体数据进行序列化和反序列化交给算子, 对数据内容一无所知
		2,既知道字段又知道字段类型(dataset和dataframe)
	优化过程:
		把sql语句转换成语法树
		对字段进行转换,转换成语法树可以识别
		把转换后的语法填充到语法树的各个位置
		按照sparksql定义的规则进行语法树的优化
			1.谓词下推	在不影响返回结果的前提下,把数据源做降量处理
			2.列值裁剪 	在不改变sql逻辑的前提下,过滤掉无用的列, 如果涉及到join,那么裁剪之后拿到的列必须包括可以与另外一张表join所需的字段
	DataFrame的创建方式:
		toDF:
		createDataFrame:
		read:
	SparkSQL 是什么????
		SparkSQL 是一个为了支持 SQL 而设计的工具, 但同时也支持命令式的 API
简略的说, RDD 有三种创建方式
	RDD 可以通过本地集合直接创建 	parallelize  parallelize方法返回的是一个ParallelCollectionRDD

	RDD 也可以通过读取外部数据集来创建  textFile  textFile方法返回的是一个HadoopRDD 

	RDD 也可以通过其它的 RDD 衍生而来   转化算子  返回的是一个MapPartitionsRDD
	
	DataSet和DataFrame区别:
		DataFrame是弱类型的(无论传过来什么类型的数据都转换为 Row 泛型), DataSet是强类型的
	两大编程范式 很强大
	
	DataSet 是一个强类型,并且类型安全的数据容器,并且提供了结构化查询api和类似rdd一样的命令式api

/总结:
	1.SparkSQL 是一个即支持 SQL 又支持命令式数据处理的工具
	2.SparkSQL 的主要适用场景是处理结构化数据
	
SparkSQL 相较于 RDD 的优势在哪???????
	1.SparkSQL 提供了更好的外部数据源读写支持
		因为大部分外部数据源是有结构化的, 需要在 RDD 之外有一个新的解决方案, 来整合这些结构化数据源

	2.SparkSQL 提供了直接访问列的能力
		因为 SparkSQL 主要用做于处理结构化数据, 所以其提供的 API 具有一些普通数据库的能力
	
Dataset 是一个强类型, 并且类型安全的数据容器, 并且提供了结构化查询 API 和类似 RDD 一样的命令式 API	


07天:
	新增列并定义数据 withcolumn
	模糊查询 like 
	枚举匹配 isin
	正反排序 .ASC 	sort
缺失值处理:
	NAN not a number
	
spark-submit --master spark://node03:7077 --class 全类名  jar包*******************
spark-submit --master spark://node01:7077 --class com.itheima.mysql.Spark_Hive  *******************




补充:
	结构化数据: 有极强的约束, 一般特指关系型数据库当中的表
	隐式转换????
	/schema (数据)结构信息
	cluster 集群
	builder()?????? 源码问题
	看视频下载的
	查看hdfs上的文件??????
	nohup /export/servers/hive-1.1.0-cdh5.14.0/bin/hive --service metastore  &
	
	selectExprs()意思???
	优化器???????????????????
	
	求一下最大值和最小值的差值, group by 一下;
	client 提交作业的服务器		container job运行的最小单位

	driver 作用;
	executor-cores 任务的并行度
	杀死yarn 任务: yarn application -kill application_id (任务的id)
	
	缺失值处理????????
	累加器和广播变量???
	项目情况???????
	
创建mysql用户:

CREATE USER 'spark'@'%' IDENTIFIED BY 'Spark123!';   'spark'@'%' 用户为spark, '%' 本地和远程都能访问,  IDENTIFIED BY 'Spark123!' 密码为 Spark123!
GRANT ALL ON spark_test.* TO 'spark'@'%';	授予所有的权限, 对spark_test这个数据库下的所有的表, 	TO 'spark'@'%'; 赋予spark这个用户, 在那个位置都能访问	

	
扩展:
	as 针对常用的类型, 1.元组 2.row对象 自定义的类型不同;
	把字符串转换为 int sparksql
	scp -r hadoop-2.7.5 node02:$PWD
	
	
	
	
	
启动spark:
	cd /export/servers/spark
	sbin/start-all.sh
	sbin/start-history-server.sh	第一天



===================================================================================================================================
08SparkStreaming:
	spark Streaming是微批次处理模型
	容错:
		sparkstreaming: 1.热备 2.冷备
	热备: 数据有冗余备份
	冷备: 预写日志 WAL
	重放:  外部数据的重放手段(kafka)

	















补充:
	复习: kafka		spark Core rdd	join算子
	rdd 静态数据集???














===================================================================================================================================
dmp01:
	大数据管理平台 dmp
	数据处理: 清洗,解析,封装数据
建仓建模:
	1. 数据经过处理后的表 (分区表)
	2. 维度表, 关联表, group by之后的表
	3. 展示的表(结果表) 
	
报表(BI) 折线图,饼图,柱状图. 工具: tableau + python 
rdd是弹性的: 内存,磁盘,或者内存和磁盘
rdd可以用foreach打印, 
collect后可以直接打印出来,可以试一下, 不能用 测试,已知数据量 (可以叫本地化)
序列化: 网络传输, 传输快



补充:
	算法....????
	flume avro source 涉及到网络传输的时候用 avro
	map(K->V,...)
	分层: 有索引的功能;
	高阶和低阶api; spark连接kafka
	
	hanlp分词器; tf-idf算法
	mongDB
	测试一下 group by 默认会按照字典排序
	
spark三部曲: 清洗,解析,封装;
资料:知网,淘宝, 咸鱼;
配置表; ..............*****************//////***********
json 一定一点剥开, list 正常
注册为临时表, DataFrame类型(表结构类型),要定义样例类



dmp04:

	广播变量: 每个rdd都能获取到 (数据量大的时候, 可以创建临时表替代) ??
	option: 
	
	

===================================================================================================================================
MongoDB:
 






	
	


	































临时函数:
create temporary function touppercase as 'cn.itcast.hive.udf.ToUpperCaseUDF';

create temporary function parseJson as 'cn.itcast.hive.udf.ParseJsonUDF';

select parseJson(text) from json_test;

insert overwrite local directory '/export/servers/hivedatas/json_test' select parseJson(text) from json_test;

load data local inpath '/export/servers/hivedatas/access.log' overwrite into table access_log;

//追加还是覆盖?? 追加
load data local inpath '/export/servers/hivedatas/score.csv'  overwrite into table score partition(month=201806);

insert overwrite local directory '/export/servers/hivedatas/sort2' select * from score sort by s_score;

insert overwrite  '/user/hive/warehouse/myhive.db/score_test' select * from score distribute by s_id sort by s_score;


//加载到hdfs上
insert overwrite directory '/user/hive/warehouse/myhive.db/score_test' row format delimited fields terminated by '\t' 
select * from score distribute by s_id sort by s_score;


高级知识:
		协同过滤算法;





	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	


